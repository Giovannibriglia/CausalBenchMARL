
from typing import Dict, List
import numpy as np
import pandas as pd
import torch

from benchmarl.models._labels import LABEL_reward_action_values


class CausalActionsFilter:
    def __init__(self, ci_online: bool, task: str, **kwargs):
        self.ci_online = ci_online
        self.task = task
        self.device = kwargs.get('device', 'cpu')
        self.path_best = f'C:\\Users\\giova\\Documents\\Research\\BenchMARL\\benchmarl\\models\\causality_best\\{self.task}'

        self.last_obs_continuous = None

        causal_table = pd.read_pickle(f'{self.path_best}/causal_table.pkl')

        self._define_action_mask_inputs(causal_table)

    def _define_action_mask_inputs(self, causal_table: pd.DataFrame):
        def actions_mask_filter(reward_action_values, possible_rewards: List, possible_actions: List):
            def weighted_action_filter(reward_action_values: Dict):
                old_min = min(possible_rewards)
                old_max = max(possible_rewards)

                # Initialize averaged_mean_dict with possible actions as float keys
                averaged_mean_dict = {float(action): 0.0 for action in possible_actions}

                for reward_value, action_probs in reward_action_values.items():
                    reward_value = float(reward_value)
                    rescaled_value = float((reward_value - old_min) / (old_max - old_min))
                    for action, prob in action_probs.items():
                        action = float(action)  # Ensure action is float
                        if action not in averaged_mean_dict:
                            raise KeyError(f"Action {action} not found in possible actions.")
                        averaged_mean_dict[action] += prob * rescaled_value

                num_reward_entries = len(reward_action_values)
                for action in averaged_mean_dict:
                    averaged_mean_dict[action] /= num_reward_entries

                return averaged_mean_dict

            action_reward_scores = weighted_action_filter(reward_action_values)
            ordered_action_reward_scores = {k: action_reward_scores[k] for k in sorted(action_reward_scores)}
            values = list(ordered_action_reward_scores.values())
            percentile_25 = np.percentile(values, 25)

            # Create actions_mask based on percentile
            actions_mask = torch.tensor([0 if value <= percentile_25 else 1 for value in values])

            # If all values are below or equal to the 25th percentile, adjust the mask
            if actions_mask.sum() == 0:
                actions_mask = torch.tensor([0 if value <= 0 else 1 for value in values])

            return actions_mask

        def process_rav(values):
            # Extract possible_rewards from the keys of the first dictionary in `values`
            possible_rewards = [float(key) for key in values[0].keys()]

            # Extract possible_actions from the keys of the first dictionary of possible_rewards
            possible_actions = [float(key) for key in values[0][str(float(possible_rewards[0]))].keys()]
            self.n_actions = len(possible_actions)

            # Directly return the tensor generated by actions_mask_filter without re-wrapping
            return [actions_mask_filter(val, possible_rewards, possible_actions).to(self.device) for val in values]

        def df_to_tensors(df: pd.DataFrame):
            dict_of_tensors = {
                str(key).replace('agent_0_obs_', ''): torch.tensor(columns_values.values, dtype=torch.float32,
                                                                   device=self.device)
                if key != LABEL_reward_action_values
                else process_rav(columns_values.values)
                for key, columns_values in df.items()
            }
            return dict_of_tensors

        self.dict_causal_table_tensors = df_to_tensors(causal_table)
        self.action_masks = self.dict_causal_table_tensors[LABEL_reward_action_values]
        self.obs_names = [str(s).replace('agent_0_obs_', '') for s in causal_table.columns.tolist() if s != LABEL_reward_action_values] # keys
        self.values_obs = [self.dict_causal_table_tensors[key] for key in self.obs_names] # values

    def get_actions(self, multiple_observation: torch.Tensor):
        def validate_input(observation: torch.Tensor) -> torch.Tensor:
            if not isinstance(observation, torch.Tensor):
                raise ValueError('multiple_observation must be a tensor')
            return observation

        def calculate_delta_obs_continuous(current_obs: torch.Tensor, last_obs: torch.Tensor) -> torch.Tensor:
            return current_obs - last_obs

        def discretize_obs(obs: torch.Tensor) -> torch.Tensor:
            obs_values = torch.stack([obs[int(name)] for name in self.obs_names])  # Ensure these values are tensors

            values_tensor = torch.stack([self.dict_causal_table_tensors[index] for index in self.obs_names])

            # Flatten the values_tensor so we can compare it to each element of obs_values
            values_tensor_flat = values_tensor.flatten()

            # Initialize an empty list to store closest values
            closest_values = []

            # For each value in obs_values, find the closest value in values_tensor_flat
            for obs_value in obs_values.squeeze(0):
                differences = torch.abs(values_tensor_flat - obs_value)  # Compute absolute differences
                min_index = torch.argmin(differences)  # Get the index of the minimum difference
                closest_value = values_tensor_flat[min_index]  # Get the closest value
                closest_values.append(closest_value)

            # Convert closest_values back to a tensor
            closest_values_tensor = torch.stack(closest_values)

            return closest_values_tensor

        def get_action_mask(obs: torch.Tensor) -> torch.Tensor:
            # Vectorized search for common indexes across all values in one go
            comparison = torch.stack(
                [self.values_obs[i] == obs[i] for i in range(len(self.obs_names))], dim=0)
            valid_indices = comparison.all(dim=0).nonzero(as_tuple=True)[0]  # Get indices where all conditions are met
            return self.action_masks[valid_indices[0]] if len(valid_indices) > 0 else torch.ones(self.n_actions,
                                                                                                 device=self.device)

        def process_obs(obs: torch.Tensor) -> torch.Tensor:
            delta_obs_cont = calculate_delta_obs_continuous(obs, self.last_obs_continuous)
            discrete_obs = discretize_obs(delta_obs_cont)
            return get_action_mask(discrete_obs)

        multiple_observation = validate_input(multiple_observation)
        num_envs, num_agents, len_obs = multiple_observation.shape

        # Flatten multiple observations for batch processing
        multiple_observation_flatten = multiple_observation.view(-1, multiple_observation.size(-1))

        if self.last_obs_continuous is not None:
            # Process all observations in a vectorized manner
            action_masks = torch.stack([process_obs(obs_input) for obs_input in multiple_observation_flatten], dim=0)
        else:
            # If no last observations, return all-ones mask
            action_masks = torch.ones((num_envs * num_agents, self.n_actions), device=self.device)

        # Store the current observations for the next call
        self.last_obs_continuous = multiple_observation_flatten
        return action_masks.view(num_envs, num_agents, -1).bool()


if __name__ == '__main__':
    online_ci = True
    causal_action_filter = CausalActionsFilter(online_ci, 'navigation')
